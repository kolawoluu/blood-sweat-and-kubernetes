# Day 5: kubelet - Understanding the Node Agent

## The Construction Site Supervisor Analogy

Imagine a construction site:
- **kubelet** = Site supervisor with a clipboard
- **Control plane** = Head office sending blueprints
- **Container runtime** = Workers with tools
- **Pods** = Buildings being constructed

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CONSTRUCTION SITE (NODE)                â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  SUPERVISOR (kubelet)              â”‚             â”‚
â”‚  â”‚  ðŸ“‹ Clipboard with instructions    â”‚             â”‚
â”‚  â”‚                                    â”‚             â”‚
â”‚  â”‚  Every 10 seconds:                 â”‚             â”‚
â”‚  â”‚  1. Check head office for orders   â”‚             â”‚
â”‚  â”‚  2. Tell workers what to build     â”‚             â”‚
â”‚  â”‚  3. Inspect buildings (health)     â”‚             â”‚
â”‚  â”‚  4. Report back to head office     â”‚             â”‚
â”‚  â”‚  5. Fix problems if found          â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚           â†“ Orders                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  WORKERS (Container Runtime)       â”‚             â”‚
â”‚  â”‚  ðŸ”¨ Actually build things          â”‚             â”‚
â”‚  â”‚  - Pull materials (images)         â”‚             â”‚
â”‚  â”‚  - Start construction (containers) â”‚             â”‚
â”‚  â”‚  - Stop work if told               â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚           â†“ Results                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  BUILDINGS (Pods/Containers)       â”‚             â”‚
â”‚  â”‚  ðŸ¢ Running applications           â”‚             â”‚
â”‚  â”‚  - nginx building                  â”‚             â”‚
â”‚  â”‚  - database building               â”‚             â”‚
â”‚  â”‚  - cache building                  â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## What is kubelet?

**kubelet = The agent running on every worker node**

**Primary responsibilities:**
1. **Watch** API Server for pods assigned to this node
2. **Start** containers via container runtime
3. **Monitor** pod and container health
4. **Report** status back to API Server
5. **Restart** failed containers
6. **Clean up** terminated pods

```
kubelet's Daily Routine:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Register node with API Server    â”‚
â”‚    "Hi, I'm node-worker-1"          â”‚
â”‚    "I have 8 CPU, 32GB RAM"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Watch API Server (every 10s)     â”‚
â”‚    "Any pods assigned to me?"       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. See pod assigned!                â”‚
â”‚    Pod: nginx-abc                   â”‚
â”‚    Image: nginx:1.21                â”‚
â”‚    Port: 80                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Tell Container Runtime           â”‚
â”‚    "Pull nginx:1.21 image"          â”‚
â”‚    "Start container"                â”‚
â”‚    "Expose port 80"                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Monitor container health         â”‚
â”‚    Every 10s:                       â”‚
â”‚    - Is container running?          â”‚
â”‚    - Health checks passing?         â”‚
â”‚    - Using too many resources?      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Report to API Server             â”‚
â”‚    "Pod nginx-abc is Running"       â”‚
â”‚    "Using 100m CPU, 200Mi memory"   â”‚
â”‚    "Health: Good âœ“"                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
     (Loop forever!)
```

---

## How kubelet Starts a Pod

### Complete Pod Lifecycle

```
API Server: "kubelet, run pod X on your node"
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: PREPARATION                                 â”‚
â”‚                                                      â”‚
â”‚ kubelet checks:                                      â”‚
â”‚ âœ“ Do I have enough CPU/memory?                      â”‚
â”‚ âœ“ Can I mount required volumes?                     â”‚
â”‚ âœ“ Are secrets/configmaps available?                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: CREATE POD SANDBOX                          â”‚
â”‚                                                      â”‚
â”‚ kubelet â†’ Container Runtime:                         â”‚
â”‚ "Create network namespace for pod"                  â”‚
â”‚                                                      â”‚
â”‚ Container Runtime:                                   â”‚
â”‚ - Creates network namespace                          â”‚
â”‚ - Sets up network (CNI plugin)                       â”‚
â”‚ - Assigns IP address to pod                          â”‚
â”‚ - Creates pause container (holds namespace)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: PULL IMAGES                                 â”‚
â”‚                                                      â”‚
â”‚ kubelet â†’ Container Runtime:                         â”‚
â”‚ "Pull image nginx:1.21"                              â”‚
â”‚                                                      â”‚
â”‚ Container Runtime:                                   â”‚
â”‚ - Checks if image exists locally (cache)             â”‚
â”‚ - If not, pulls from registry                        â”‚
â”‚ - Downloads layers                                   â”‚
â”‚ - Extracts filesystem                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4: CREATE CONTAINERS                           â”‚
â”‚                                                      â”‚
â”‚ For each container in pod:                           â”‚
â”‚ kubelet â†’ Container Runtime:                         â”‚
â”‚ "Create container with these settings"               â”‚
â”‚                                                      â”‚
â”‚ Container Runtime:                                   â”‚
â”‚ - Creates container from image                       â”‚
â”‚ - Sets resource limits (CPU/memory)                  â”‚
â”‚ - Mounts volumes                                     â”‚
â”‚ - Sets environment variables                         â”‚
â”‚ - Configures security context                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 5: START CONTAINERS                            â”‚
â”‚                                                      â”‚
â”‚ kubelet â†’ Container Runtime:                         â”‚
â”‚ "Start the container"                                â”‚
â”‚                                                      â”‚
â”‚ Container Runtime:                                   â”‚
â”‚ - Runs container process                             â”‚
â”‚ - Container starts executing                         â”‚
â”‚ - nginx process starts listening on port 80          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 6: HEALTH CHECKS                               â”‚
â”‚                                                      â”‚
â”‚ kubelet runs probes:                                 â”‚
â”‚ - Startup probe (is app starting?)                   â”‚
â”‚ - Readiness probe (is app ready for traffic?)       â”‚
â”‚ - Liveness probe (is app still healthy?)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 7: UPDATE STATUS                               â”‚
â”‚                                                      â”‚
â”‚ kubelet â†’ API Server:                                â”‚
â”‚ "Pod is Running"                                     â”‚
â”‚ "IP: 10.244.1.5"                                     â”‚
â”‚ "Ready: True"                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total time: Usually 5-30 seconds
(Depends on image size and pull time)
```

---

## Key Concepts

### 1. Pod Sandbox (The pause container)

**What is it?** Every pod has a "pause" container that does nothing!

```
Why the pause container?

Problem without it:
- Pod has 2 containers: nginx + sidecar
- nginx starts first, gets network namespace
- nginx crashes and restarts
- New nginx gets different network namespace
- Pod IP changes! âŒ
- Service endpoints break!

Solution with pause container:
- pause container starts first
- Creates and holds network namespace
- Gets pod IP: 10.244.1.5
- nginx joins this namespace
- sidecar joins this namespace
- nginx crashes and restarts
- Rejoins same namespace
- Pod IP unchanged! âœ“

The pause container:
- Does nothing (literally just sleeps)
- Never crashes
- Keeps network namespace alive
- All pod containers share its namespace
```

**Real example:**
```bash
docker ps

CONTAINER ID   IMAGE                  COMMAND
abc123         nginx:1.21             "nginx"
def456         fluent/fluentd         "fluentd"  
xyz789         k8s.gcr.io/pause:3.5   "/pause"  â† This!

The pause container keeps the pod's network alive!
```

---

### 2. PLEG (Pod Lifecycle Event Generator)

**What is PLEG?** kubelet's way of knowing what's happening with containers

```
PLEG is like a motion detector:

Every 1 second:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PLEG â†’ Container Runtime:           â”‚
â”‚ "What containers are running?"      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Container Runtime responds:         â”‚
â”‚ - Container A: Running              â”‚
â”‚ - Container B: Exited               â”‚
â”‚ - Container C: Started (new!)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PLEG detects changes:               â”‚
â”‚ - Container B exited! (was running) â”‚
â”‚ - Container C started! (was absent) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ kubelet reacts:                     â”‚
â”‚ - Container B: Restart it           â”‚
â”‚ - Container C: Start health checks  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

If PLEG fails:
- kubelet can't detect container state changes
- Can't restart crashed containers
- Node becomes "NotReady"
- Disaster! ðŸ’¥
```

**Real incident - Shopify Black Friday:**
```
Timeline:

23:50 - Black Friday begins
        - Massive traffic spike
        - Disk I/O saturates

23:55 - Container runtime slow to respond
        - PLEG polls take >3 seconds (should be <100ms)
        - PLEG timeout: "Container runtime not responding!"

00:00 - PLEG marked unhealthy
        - Node marked NotReady
        - No new pods scheduled to node
        - But existing pods still running!

00:05 - 50 nodes affected
        - 50% of cluster capacity lost
        - Remaining nodes overloaded

00:10 - Engineers investigate
        - Disk I/O at 100%
        - Container runtime queued up requests
        - PLEG timing out

00:15 - Fix: Throttle disk I/O
        - Reduce container logs
        - PLEG responds fast again
        - Nodes become Ready

00:30 - Full recovery

Lesson: PLEG health = Node health
Fast disks = Critical!
```

---

### 3. Health Checks (Probes)

**Three types of probes:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. STARTUP PROBE                                     â”‚
â”‚    "Is the app starting?"                            â”‚
â”‚                                                      â”‚
â”‚    Use when: App takes long to start (1+ minutes)   â”‚
â”‚    Example: ML model loading                         â”‚
â”‚                                                      â”‚
â”‚    startup probe:                                    â”‚
â”‚      httpGet: /started                               â”‚
â”‚      initialDelaySeconds: 0                          â”‚
â”‚      periodSeconds: 10                               â”‚
â”‚      failureThreshold: 30  (try for 5 minutes)      â”‚
â”‚                                                      â”‚
â”‚    If fails: Container killed and restarted          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. READINESS PROBE                                   â”‚
â”‚    "Is app ready to serve traffic?"                  â”‚
â”‚                                                      â”‚
â”‚    Use when: App healthy but temporarily busy        â”‚
â”‚    Example: Database connection pool full            â”‚
â”‚                                                      â”‚
â”‚    readiness probe:                                  â”‚
â”‚      httpGet: /ready                                 â”‚
â”‚      periodSeconds: 5                                â”‚
â”‚      failureThreshold: 2                             â”‚
â”‚                                                      â”‚
â”‚    If fails: Removed from Service endpoints          â”‚
â”‚    Container NOT killed, keeps running               â”‚
â”‚    When passes again: Added back to Service          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. LIVENESS PROBE                                    â”‚
â”‚    "Is app still healthy?"                           â”‚
â”‚                                                      â”‚
â”‚    Use when: App can get stuck/deadlocked            â”‚
â”‚    Example: Infinite loop, memory leak               â”‚
â”‚                                                      â”‚
â”‚    liveness probe:                                   â”‚
â”‚      httpGet: /health                                â”‚
â”‚      periodSeconds: 10                               â”‚
â”‚      failureThreshold: 3                             â”‚
â”‚                                                      â”‚
â”‚    If fails: Container killed and restarted          â”‚
â”‚    Last resort! Don't overuse!                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Probe Methods

```
1. HTTP GET
   httpGet:
     path: /health
     port: 8080
   
   kubelet makes HTTP request
   Success: Status 200-399
   Failure: Other status codes or timeout

2. TCP Socket
   tcpSocket:
     port: 3306
   
   kubelet tries to open TCP connection
   Success: Connection established
   Failure: Connection refused or timeout

3. Exec Command
   exec:
     command:
     - cat
     - /tmp/healthy
   
   kubelet runs command in container
   Success: Exit code 0
   Failure: Non-zero exit code
```

---

### Real Example - E-commerce Checkout

```
Checkout service pod:

containers:
- name: checkout
  image: checkout:v2
  
  # Takes 30 seconds to load product catalog
  startupProbe:
    httpGet:
      path: /startup
      port: 8080
    periodSeconds: 5
    failureThreshold: 10  # 50 seconds max
  
  # Ready when database connection pool available
  readinessProbe:
    httpGet:
      path: /ready
      port: 8080
    periodSeconds: 3
    successThreshold: 1
    failureThreshold: 2   # 6 seconds of failures
  
  # Liveness for deadlock detection
  livenessProbe:
    httpGet:
      path: /health
      port: 8080
    periodSeconds: 10
    failureThreshold: 3   # 30 seconds of failures

Scenario 1: Normal startup
0s  - Container starts
0s  - Startup probe: Fail (app loading)
5s  - Startup probe: Fail (still loading)
10s - Startup probe: Fail
15s - Startup probe: Fail
20s - Startup probe: Fail
25s - Startup probe: Fail
30s - Startup probe: Pass! âœ“ (app loaded)
30s - Readiness probe: Fail (connecting to DB)
33s - Readiness probe: Pass! âœ“ (DB connected)
     - Pod added to Service endpoints
     - Starts receiving traffic!

Scenario 2: Database down
Pod running normally
Receiving traffic
       â†“
Database goes down
       â†“
0s  - Readiness probe: Fail (can't connect to DB)
3s  - Readiness probe: Fail
6s  - Pod removed from Service âœ“
     - No new traffic sent to this pod
     - But container keeps running!
       â†“
Database comes back
       â†“
0s  - Readiness probe: Pass! âœ“
     - Pod added back to Service
     - Starts receiving traffic again

Scenario 3: App deadlock
Pod running, receiving traffic
       â†“
App deadlocks (bug in code)
Responds to readiness: OK (quick check)
But can't actually process requests
       â†“
0s  - Liveness probe: Timeout (deadlocked)
10s - Liveness probe: Timeout
20s - Liveness probe: Timeout
30s - 3 failures! Kill container!
     - kubelet kills container
     - kubelet starts new container
     - Fresh start, no deadlock
     - Service resumes!
```

---

## What Happens When kubelet Dies?

### Immediate Impact

```
kubelet crashes on node
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ IMMEDIATE (0-40 seconds):               â”‚
â”‚                                         â”‚
â”‚ âœ“ Containers keep running               â”‚
â”‚   (Container runtime independent)       â”‚
â”‚                                         â”‚
â”‚ âœ“ Services keep working                 â”‚
â”‚   (kube-proxy still routing)            â”‚
â”‚                                         â”‚
â”‚ âŒ Can't start new pods                 â”‚
â”‚    (No kubelet to talk to runtime)      â”‚
â”‚                                         â”‚
â”‚ âŒ Can't restart crashed containers     â”‚
â”‚    (No health monitoring)               â”‚
â”‚                                         â”‚
â”‚ âŒ Can't report status                  â”‚
â”‚    (API Server doesn't know pod state)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AFTER 40 SECONDS:                       â”‚
â”‚                                         â”‚
â”‚ Node Controller notices:                â”‚
â”‚ "Node hasn't reported in 40 seconds"   â”‚
â”‚                                         â”‚
â”‚ Node marked: Ready â†’ NotReady           â”‚
â”‚                                         â”‚
â”‚ Scheduler: "Don't send new pods there" â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AFTER 5 MINUTES:                        â”‚
â”‚                                         â”‚
â”‚ Node Controller evicts pods:            â”‚
â”‚ "Node still not responding"             â”‚
â”‚                                         â”‚
â”‚ Pods marked: Terminating                â”‚
â”‚                                         â”‚
â”‚ ReplicaSet Controller:                  â”‚
â”‚ "Create replacement pods on other nodes"â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

But! Containers still running on dead node!
This creates "zombie pods" - running but unknown to cluster
```

---

### Real Incident - Target (2018)

```
Timeline:

14:00 - Normal operations
        - 200 worker nodes
        - 5000 pods running

14:15 - kubelet memory leak discovered
        - Affects 50 nodes
        - kubelet memory: 8GB â†’ 16GB â†’ 32GB

14:30 - Nodes start OOMKilling kubelet
        - kubelet dies, can't restart (no memory)
        - 50 nodes affected

14:32 - Pods on these nodes: Status Unknown
        - But containers still running!
        - Zombie pods consuming resources

14:37 - Node Controller marks nodes NotReady
        - No new pods scheduled there

14:42 - Node Controller evicts pods (after 5 min)
        - Expects pods to terminate
        - But no kubelet to terminate them!
        - Pods still running as zombies!

14:45 - ReplicaSet Controllers create replacements
        - 1250 new pods created (50 nodes Ã— 25 avg)
        - Scheduled to healthy nodes
        - Healthy nodes now overloaded!

15:00 - Cascading failures
        - Healthy nodes overwhelmed
        - Some healthy nodes' kubelet OOM
        - More zombie pods created
        - Snowball effect!

15:30 - Emergency measures
        - Manually drain nodes
        - SSH in, forcefully kill containers
        - Restart nodes with fresh kubelet

16:30 - Partial recovery
        - 150 nodes healthy
        - 50 nodes being recovered

18:00 - Full recovery

Total impact: 4 hours
Root cause: kubelet memory leak + no resource limits
Lost revenue: Estimated $500k

Lessons learned:
1. Set resource limits on kubelet
2. Monitor kubelet health
3. Automated zombie pod cleanup
4. Better node lifecycle management
```

---

## Container Runtime Interface (CRI)

### How kubelet Talks to Container Runtime

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         kubelet                         â”‚
â”‚   (Doesn't care which runtime!)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“ CRI API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Container Runtime Interface (CRI)     â”‚
â”‚   (Standard API for container ops)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
        â†“             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ containerd   â”‚ â”‚ CRI-O        â”‚
â”‚ (Docker's    â”‚ â”‚ (Red Hat)    â”‚
â”‚  successor)  â”‚ â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CRI API Operations:
- RunPodSandbox() - Create pod network
- CreateContainer() - Create container
- StartContainer() - Start container
- StopContainer() - Stop container
- RemoveContainer() - Delete container
- ListPodSandbox() - List pods (PLEG uses this!)
- PodSandboxStatus() - Get pod status
- ContainerStatus() - Get container status
```

---

### Why CRI Matters

```
Before CRI (old days):
kubelet hardcoded to use Docker
Want to use rkt? Can't!
Want to use another runtime? Can't!

With CRI (now):
kubelet uses standard CRI API
Any runtime that implements CRI works!
- containerd âœ“
- CRI-O âœ“
- Kata Containers âœ“
- gVisor âœ“

Flexibility! Choose runtime for your needs:
- containerd: Standard, widely used
- CRI-O: Lightweight, Kubernetes-native
- Kata Containers: VM-based, more secure
- gVisor: Sandboxed, extra security
```

---

## Resource Management

### How kubelet Enforces Resource Limits

```
Pod requests:
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 1000m
      memory: 2Gi

kubelet translates to cgroups:

1. CPU Limits (via cpu.cfs_quota_us):
   1000m = 1 core = 100,000 microseconds per 100ms
   
   If container tries to use >1 core:
   - Gets throttled
   - Slowed down
   - But keeps running

2. Memory Limits (via memory.limit_in_bytes):
   2Gi = 2,147,483,648 bytes
   
   If container tries to use >2Gi:
   - Gets OOMKilled immediately!
   - Container terminated
   - kubelet restarts it

Why different?
- CPU: Can share/throttle (not destructive)
- Memory: Can't take back (must kill)
```

---

### Real Example - Memory Leak

```
Application with memory leak:

Start: 100Mi memory
After 1 hour: 500Mi
After 2 hours: 1Gi
After 3 hours: 2Gi (limit!)
       â†“
Container tries to allocate more
       â†“
Linux OOM Killer:
"Container exceeded memory limit!"
       â†“
Kills container process
       â†“
kubelet detects: Container exited
       â†“
kubelet: "Restart container" (restart policy)
       â†“
New container starts: 100Mi
       â†“
Leak starts again...
       â†“
After 3 hours: OOMKilled again
       â†“
This repeats forever!

Pod status shows:
NAME              READY   RESTARTS   AGE
leaky-app-abc     1/1     47         3d

47 restarts = leaking 47 times!

Solution:
- Fix the leak (best)
- Or increase memory limit
- Or set restart policy: Never (let it die)
```

---

## Static Pods

### Special Pods Managed by kubelet

```
What are static pods?
- Defined in files on node
- Not in etcd!
- kubelet manages them directly
- No API Server needed!

Location: /etc/kubernetes/manifests/

Example: Control plane components
- kube-apiserver.yaml
- kube-controller-manager.yaml
- kube-scheduler.yaml
- etcd.yaml

kubelet watches this directory:
- File added? Start pod
- File modified? Restart pod
- File removed? Stop pod

Why useful?
- Bootstrap problem: API Server needs to run
- But API Server is itself a pod
- Static pod solves chicken-and-egg!

Flow:
1. kubelet starts (no API Server yet)
2. kubelet reads /etc/kubernetes/manifests/
3. kubelet starts kube-apiserver.yaml
4. API Server pod starts
5. Now API Server available!
6. Other components can start normally
```

---

### Static Pod Trick

```
Create static pod:
echo "
apiVersion: v1
kind: Pod
metadata:
  name: my-static-pod
spec:
  containers:
  - name: nginx
    image: nginx
" > /etc/kubernetes/manifests/my-static-pod.yaml

kubelet automatically:
- Sees new file
- Starts pod
- Registers with API Server (if available)

kubectl get pods
NAME                        READY   STATUS
my-static-pod-node1         1/1     Running

Try to delete:
kubectl delete pod my-static-pod-node1
pod "my-static-pod-node1" deleted

Check again:
kubectl get pods
NAME                        READY   STATUS
my-static-pod-node1         1/1     Running

It's back! kubelet recreated it (file still exists)

To actually delete:
rm /etc/kubernetes/manifests/my-static-pod.yaml

Now it's gone for real!
```

---

## Key Takeaways

### The 5 Most Important Concepts

**1. kubelet = Node Agent**
```
One kubelet per node
Responsibilities:
- Register node
- Watch for pods assigned to node
- Start containers
- Monitor health
- Report status
- Restart failures

It's the "worker" of Kubernetes!
```

**2. PLEG is Critical**
```
PLEG = Pod Lifecycle Event Generator
Polls container runtime every 1 second
Detects container state changes

If PLEG unhealthy:
- Node marked NotReady
- No new pods scheduled
- Existing pods keep running (usually)

Fast disks = Healthy PLEG!
```

**3. Three Types of Probes**
```
Startup: Is app starting? (kills if fails)
Readiness: Ready for traffic? (removes from Service if fails)
Liveness: Still healthy? (kills if fails)

Best practice:
- Always set readiness
- Use liveness sparingly
- Set generous timeouts
```

**4. Resource Limits are Enforced**
```
CPU limit: Throttled (slowed down)
Memory limit: OOMKilled (terminated)

Set limits on all pods!
Prevents one pod from starving others
```

**5. kubelet Failure = Serious**
```
No kubelet:
- Can't start new pods
- Can't restart crashed containers
- Can't report status
- Creates zombie pods

Monitor kubelet health!
Set resource limits on kubelet itself!
```

---

## Tomorrow (Day 6)

We'll explore:
- Kubernetes networking
- CNI plugins
- kube-proxy and Services
- DNS (CoreDNS)
- How pods communicate
- Network troubleshooting

**You now understand kubelet - the hardworking agent on every node that actually runs your containers! Tomorrow we'll see how these containers talk to each other across the network.** ðŸ”Œ