# Day 2: API Server & etcd - Understanding the Brain and Memory

## The Library Analogy

Imagine a huge library system:
- **API Server** = The librarian (front desk person)
- **etcd** = The actual library shelves storing all books
- **Controllers** = Library patrons reading books and taking actions

---

## API Server - The Smart Librarian

### What Does API Server Do?

**Analogy:** The world's strictest but most helpful librarian

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              YOU (via kubectl)                      â”‚
â”‚         "I want to borrow 'Pod' book"               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  API SERVER                         â”‚
â”‚                                                     â”‚
â”‚  Step 1: AUTHENTICATION                             â”‚
â”‚  "Who are you? Show me your library card!"          â”‚
â”‚  âœ“ Valid token/certificate                          â”‚
â”‚                                                     â”‚
â”‚  Step 2: AUTHORIZATION                              â”‚
â”‚  "Are you allowed to borrow this book?"             â”‚
â”‚  âœ“ Check RBAC rules                                 â”‚
â”‚                                                     â”‚
â”‚  Step 3: VALIDATION                                 â”‚
â”‚  "Is this a valid book request?"                    â”‚
â”‚  âœ“ Check if request makes sense                     â”‚
â”‚                                                     â”‚
â”‚  Step 4: PERSISTENCE                                â”‚
â”‚  "Let me write this down in the records..."         â”‚
â”‚  â†’ Saves to etcd                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     etcd                            â”‚
â”‚           (The library shelves)                     â”‚
â”‚  Stores: "User X borrowed Pod book at 10:00 AM"    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Why API Server is Special

**1. Single Point of Entry**
- Every request goes through API Server
- No backdoors! (Security)
- This includes:
  - Your kubectl commands
  - Controller Manager checking state
  - Scheduler assigning pods
  - kubelet reporting status

**2. Stateless**
- API Server has no memory of its own
- Gets all information from etcd
- Can run multiple API Servers (high availability)
- If one dies, use another!

**3. RESTful API**
```
GET    /api/v1/pods          â†’ List all pods
POST   /api/v1/pods          â†’ Create new pod
DELETE /api/v1/pods/nginx-1  â†’ Delete specific pod
PATCH  /api/v1/pods/nginx-1  â†’ Update pod
```

---

### How API Server Processes Requests

**Example: Creating a Pod**

```
Step-by-Step: kubectl create pod nginx

1. kubectl â†’ API Server
   POST /api/v1/namespaces/default/pods
   {
     "name": "nginx",
     "image": "nginx:latest"
   }

2. API Server: Authentication
   â†’ Checks your kubeconfig token
   â†’ "Valid user? YES âœ“"

3. API Server: Authorization  
   â†’ Checks RBAC rules
   â†’ "Can this user create pods? YES âœ“"

4. API Server: Admission Controllers
   â†’ LimitRanger: "Does pod have resource limits?"
   â†’ PodSecurity: "Is pod configuration secure?"
   â†’ Many other checks...
   â†’ ALL PASS âœ“

5. API Server: Validation
   â†’ Schema validation
   â†’ "Is this valid YAML/JSON? YES âœ“"
   â†’ "Do all required fields exist? YES âœ“"

6. API Server â†’ etcd
   â†’ Writes pod definition to database
   â†’ etcd returns: "Saved successfully"

7. API Server â†’ You
   â†’ Returns: "Pod created!"

8. Controllers watching etcd see new pod
   â†’ Scheduler: "New pod needs node assignment!"
   â†’ Assigns to node
   
9. kubelet on that node sees pod assigned
   â†’ Starts the pod
```

**Time taken:** Usually 100-500ms for entire flow!

---

## etcd - The Cluster's Memory

### What is etcd?

**Analogy:** Like a super-organized filing cabinet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    etcd                          â”‚
â”‚         (Distributed Key-Value Store)            â”‚
â”‚                                                  â”‚
â”‚  ğŸ“ /registry/pods/                              â”‚
â”‚     â””â”€ default/                                  â”‚
â”‚        â”œâ”€ nginx-1    â†’ {pod details}             â”‚
â”‚        â”œâ”€ nginx-2    â†’ {pod details}             â”‚
â”‚        â””â”€ nginx-3    â†’ {pod details}             â”‚
â”‚                                                  â”‚
â”‚  ğŸ“ /registry/services/                          â”‚
â”‚     â””â”€ default/                                  â”‚
â”‚        â””â”€ web        â†’ {service details}         â”‚
â”‚                                                  â”‚
â”‚  ğŸ“ /registry/deployments/                       â”‚
â”‚     â””â”€ default/                                  â”‚
â”‚        â””â”€ app        â†’ {deployment details}      â”‚
â”‚                                                  â”‚
â”‚  ğŸ“ /registry/secrets/                           â”‚
â”‚     â””â”€ default/                                  â”‚
â”‚        â””â”€ db-pass    â†’ {base64 encoded}          â”‚
â”‚                                                  â”‚
â”‚  ğŸ“ /registry/configmaps/                        â”‚
â”‚  ğŸ“ /registry/replicasets/                       â”‚
â”‚  ğŸ“ /registry/nodes/                             â”‚
â”‚  ... (everything in your cluster!)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### How etcd Works

**Key Concepts:**

**1. Key-Value Store**
```
Key: /registry/pods/default/nginx
Value: {
  "apiVersion": "v1",
  "kind": "Pod",
  "metadata": {
    "name": "nginx",
    "namespace": "default"
  },
  "spec": {
    "containers": [{
      "name": "nginx",
      "image": "nginx:1.21"
    }]
  }
}
```

**2. Distributed & Replicated**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ etcd-1  â”‚ â†â”€â”€â†’ â”‚ etcd-2  â”‚ â†â”€â”€â†’ â”‚ etcd-3  â”‚
â”‚ Leader  â”‚      â”‚ Followerâ”‚      â”‚ Followerâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                â”‚                â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Raft Consensus
```

- Run 3 or 5 etcd instances (always odd number)
- One is Leader, others are Followers
- Leader handles writes
- All must agree before write is confirmed (majority vote)
- If Leader dies, new election happens (~seconds)

**3. Consistent & Durable**
- Every write is persisted to disk
- Every write is replicated to majority
- If you write something, it's GUARANTEED to be there
- This is why etcd needs fast SSDs!

---

### Why etcd is Critical

**etcd stores EVERYTHING:**
- Every pod, service, deployment
- All configurations
- All secrets (base64 encoded)
- Cluster state
- Node information

**If etcd dies:**
```
Before:
- API Server knows: "5 pods are running on node-1"
- Controllers know: "I should maintain 5 pods"

After etcd crash:
- API Server: "I don't know what exists!" ğŸ˜±
- Controllers: "I don't know desired state!" ğŸ˜±
- But: kubelet keeps running existing pods âœ“
```

Cluster becomes READ-ONLY and eventually UNUSABLE!

---

### etcd Performance Characteristics

**What makes etcd fast:**
- In-memory reads (milliseconds)
- WAL (Write-Ahead Log) for durability
- Batched writes
- Compaction to prevent disk growth

**What makes etcd slow:**
- Too many writes (thousands per second)
- Slow disks (HDD instead of SSD)
- Network latency between etcd nodes
- Large objects (secrets >1MB)

**Real numbers:**
- Good: 1000 writes/sec on SSD
- Bad: 100 writes/sec on HDD
- Disaster: 10 writes/sec on network storage

---

## Real-World Examples

### Example 1: Netflix Deployment

**Scenario:** Netflix deploys new version of video streaming service

```
1. Engineer: kubectl apply -f streaming-service.yaml

2. API Server receives request
   - Authenticates: Netflix engineer âœ“
   - Authorizes: Can deploy to production? âœ“
   - Validates: YAML syntax correct? âœ“
   
3. API Server â†’ etcd
   - Writes new deployment spec
   - etcd replicates to 3 nodes
   - Returns: "Write confirmed"
   
4. Deployment Controller watches etcd
   - Sees: "New deployment, needs 100 pods"
   - Creates 100 pod definitions
   
5. Scheduler watches etcd
   - Sees: "100 pods need placement"
   - Assigns pods to nodes based on:
     * Available CPU/memory
     * Geographic distribution
     * Pod anti-affinity rules
   
6. kubelets watch etcd
   - See: "I have 10 pods assigned to me"
   - Start containers
   
7. All states saved in etcd continuously
```

**Why this matters:**
- etcd stores: deployment config, pod assignments, node status
- If engineer asks "Where is my deployment?", API Server reads from etcd
- If node crashes, Controller reads from etcd, creates replacement pods

---

### Example 2: Airbnb Auto-Scaling

**Scenario:** Traffic spike during holiday weekend

```
1. Metrics show high CPU usage

2. HPA (HorizontalPodAutoscaler) decides:
   - "Need to scale from 10 pods to 50 pods"
   
3. HPA â†’ API Server
   - PATCH /apis/apps/v1/deployments/booking-service
   - Change replicas: 10 â†’ 50
   
4. API Server â†’ etcd
   - Updates deployment spec in etcd
   - etcd confirms write
   
5. Deployment Controller watches etcd
   - Sees: "Replicas changed from 10 to 50"
   - Creates 40 new pods
   
6. Scheduler watches etcd
   - Assigns 40 new pods to nodes
   - Spreads across availability zones
   
7. kubelets start 40 new containers

All within ~30 seconds!
```

---

### Example 3: Spotify Disaster Recovery

**Scenario:** Data center power failure

```
Before disaster:
- 3 etcd nodes: DC1, DC2, DC3
- DC1 is Leader
- All data replicated

During disaster:
- DC1 loses power (Leader down!)
- DC2 and DC3 still running
- New election: DC2 becomes Leader (takes ~5 seconds)
- Cluster continues working! âœ“

After recovery:
- DC1 comes back online
- Rejoins as Follower
- Catches up on missed data
- Cluster fully healthy again
```

**Why this works:**
- Raft consensus: Need majority (2 out of 3)
- Even with 1 etcd down, cluster survives
- This is why you run 3 or 5 etcd nodes!

---

## API Server & etcd Relationship

### The Dance Between Them

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   API SERVER                           â”‚
â”‚                                                        â”‚
â”‚  Watch Cache (in-memory):                              â”‚
â”‚  â”œâ”€ Pods cache                                         â”‚
â”‚  â”œâ”€ Services cache                                     â”‚
â”‚  â””â”€ Deployments cache                                  â”‚
â”‚                                                        â”‚
â”‚  Why cache?                                            â”‚
â”‚  - Faster reads (memory vs disk)                       â”‚
â”‚  - Less load on etcd                                   â”‚
â”‚  - Updated by etcd watch streams                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†• Bidirectional Communication
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      etcd                              â”‚
â”‚                                                        â”‚
â”‚  Write Path:                                           â”‚
â”‚  API Server â†’ etcd: Save new data                      â”‚
â”‚                                                        â”‚
â”‚  Read Path:                                            â”‚
â”‚  API Server â†’ etcd: Get initial data                   â”‚
â”‚                                                        â”‚
â”‚  Watch Path:                                           â”‚
â”‚  etcd â†’ API Server: "Data changed! Update cache"       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### What Happens on API Server Crash

```
API Server Crashes:
       â†“
New requests fail (kubectl commands timeout)
       â†“
Controllers can't read/write state
       â†“
But: Other API Server instances take over! (if HA)
       â†“
Or: kubelet restarts API Server (static pod)
       â†“
Cluster resumes normal operation
```

**Production Setup:**
- Run 3+ API Servers behind load balancer
- If one crashes, others handle traffic
- No single point of failure

---

### What Happens on etcd Crash

```
One etcd Node Crashes:
       â†“
If you have 3 nodes total:
- 2 still running = Majority âœ“
- Cluster continues working
       â†“
Write example:
- API Server writes to Leader
- Leader replicates to 1 Follower (2 total)
- That's majority (2 out of 3) âœ“
- Write confirmed!

All etcd Nodes Crash:
       â†“
NO MAJORITY = Cluster DEAD ğŸ’€
       â†“
API Server can't read or write
       â†“
kubectl commands fail
       â†“
Controllers stop working
       â†“
But: Existing pods keep running!
       â†“
Need to restore from backup
```

---

## etcd Data Organization

### How Kubernetes Objects are Stored

```
etcd Key Structure:
/registry/<resource-type>/<namespace>/<name>

Examples:
/registry/pods/default/nginx-abc123
/registry/services/production/web-api
/registry/deployments/staging/app-v2
/registry/secrets/kube-system/admin-token
/registry/configmaps/default/app-config
/registry/namespaces/production
/registry/nodes/worker-node-1
```

### Actual etcd Data Example

**Pod stored in etcd:**
```json
Key: /registry/pods/default/nginx-abc123

Value: {
  "kind": "Pod",
  "apiVersion": "v1",
  "metadata": {
    "name": "nginx-abc123",
    "namespace": "default",
    "uid": "12345-67890-abcdef",
    "resourceVersion": "12345",
    "creationTimestamp": "2024-10-12T10:00:00Z",
    "labels": {
      "app": "nginx"
    }
  },
  "spec": {
    "containers": [{
      "name": "nginx",
      "image": "nginx:1.21",
      "ports": [{"containerPort": 80}]
    }],
    "nodeName": "worker-node-2"
  },
  "status": {
    "phase": "Running",
    "podIP": "10.244.1.5",
    "startTime": "2024-10-12T10:00:05Z"
  }
}
```

**Why this structure matters:**
- Fast lookups: `/registry/pods/default/` lists all pods in namespace
- Hierarchical: Easy to query by namespace
- Versioned: `resourceVersion` tracks changes
- Complete: Everything needed to recreate pod

---

## Backup & Disaster Recovery

### Why Backup etcd?

**Scenario: Complete Cluster Loss**

```
Without Backup:
âŒ All pod definitions lost
âŒ All service configurations lost  
âŒ All secrets lost
âŒ All custom resources lost
âŒ Must manually recreate everything
âŒ Hours/days of work

With Backup:
âœ… Restore etcd snapshot
âœ… All configurations back instantly
âœ… Redeploy applications
âœ… Cluster operational in minutes
```

---

### How etcd Backup Works

```
Backup Process:
1. etcdctl snapshot save /backup/etcd-snapshot.db
   - Creates point-in-time snapshot
   - Captures entire etcd database
   - Typically 100MB - 2GB size

2. Copy snapshot to safe location
   - Cloud storage (S3, GCS)
   - Different data center
   - Multiple copies

3. Schedule regular backups
   - Hourly: for very critical clusters
   - Daily: for most production clusters
   - Weekly: for dev/test clusters
```

---

### Real Disaster Recovery Example: GitLab.com (2017)

**What Happened:**
1. Engineer accidentally deletes production database
2. Primary database gone
3. Realize backups are broken!
4. Panic! ğŸ˜±

**Recovery:**
```
Hour 0: Database deleted
     â†“
Hour 1: Discover backups not working
     â†“
Hour 2: Find one working backup (6 hours old)
     â†“
Hour 3: Restore etcd from snapshot
     â†“
Hour 4: Restore application databases
     â†“
Hour 5: Systems back online
     â†“
Result: Lost 6 hours of data
Lesson: TEST YOUR BACKUPS!
```

**GitLab's Changes After:**
- Automated backup testing
- Multiple backup locations
- More frequent backups
- Better access controls

---

## Performance & Scalability

### etcd Performance Limits

**Small Cluster (100 nodes):**
- etcd size: ~500 MB
- Objects: ~5,000 pods
- Writes/sec: 500
- Reads/sec: 10,000
- Performance: Excellent âœ“

**Medium Cluster (500 nodes):**
- etcd size: ~2 GB
- Objects: ~25,000 pods
- Writes/sec: 1,000
- Reads/sec: 50,000
- Performance: Good âœ“

**Large Cluster (5,000 nodes):**
- etcd size: ~8 GB
- Objects: ~150,000 pods
- Writes/sec: 2,000
- Reads/sec: 100,000+
- Performance: Requires tuning âš ï¸

**At Scale Issues:**
- etcd database size grows
- More compaction needed
- Network bandwidth matters
- Need faster disks (NVMe SSD)

---

### API Server Performance

**What Slows Down API Server:**

1. **Too many requests**
```
Normal: 1,000 requests/sec â†’ Fast âœ“
High: 10,000 requests/sec â†’ Slow âš ï¸
Extreme: 50,000 requests/sec â†’ Crash ğŸ’¥
```

2. **Large objects**
```
ConfigMap with 1 KB data â†’ Fast âœ“
ConfigMap with 1 MB data â†’ Slow âš ï¸
Secret with 5 MB data â†’ Very slow ğŸ’¥
```

3. **List all pods**
```
List 100 pods â†’ 50ms âœ“
List 10,000 pods â†’ 5 seconds âš ï¸
List 100,000 pods â†’ 30+ seconds ğŸ’¥
```

**Solutions:**
- Use pagination for large lists
- Cache frequently accessed data
- Run multiple API Server instances
- Use filters to reduce data returned

---

## Real-World Use Cases

### Use Case 1: Microservices Platform (Uber)

**Challenge:**
- 4,000+ microservices
- 100,000+ pods
- Constant deployments (every 2 minutes)

**How API Server & etcd Handle It:**
```
Deployment Flow:
1. Engineer pushes code
2. CI/CD â†’ API Server: Deploy new version
3. API Server â†’ etcd: Update deployment
4. Deployment Controller â†’ Creates new pods
5. Scheduler â†’ Assigns to nodes
6. Old pods â†’ Gracefully terminated

All data flows through API Server â†’ etcd

etcd Optimizations:
- 5 etcd nodes (high availability)
- NVMe SSDs (fast writes)
- Dedicated etcd servers (no sharing)
- Hourly backups to 3 locations
```

---

### Use Case 2: Multi-Tenant Platform (Shopify)

**Challenge:**
- Thousands of merchant stores
- Each needs isolated environment
- Dynamic scaling
- Strict security

**How It Works:**
```
Per Merchant:
- Namespace in Kubernetes
- ResourceQuotas (limits resources)
- NetworkPolicies (isolates network)
- RBAC (controls access)

All stored in etcd:
/registry/namespaces/merchant-123/
â”œâ”€ pods/
â”œâ”€ services/
â”œâ”€ resourcequotas/
â”œâ”€ networkpolicies/
â””â”€ rolebindings/

API Server ensures:
- Merchant A can't see Merchant B's data
- Each merchant stays within quota
- Network isolation enforced
```

---

### Use Case 3: Gaming Platform (Epic Games - Fortnite)

**Challenge:**
- Massive traffic spikes
- Game lobbies created instantly
- Each lobby = set of pods
- Need fast scaling

**How It Works:**
```
Player Joins Game:
1. API call â†’ Create lobby
       â†“
2. API Server â†’ etcd: Save lobby config
       â†“
3. Controller â†’ Creates lobby pods:
   - Game server pod
   - Voice chat pod
   - Stats pod
       â†“
4. Scheduler â†’ Places on low-latency nodes
       â†“
5. Player connected in ~2 seconds

Peak Traffic:
- 10,000 lobbies/minute
- 30,000 new pods/minute
- etcd handles: 2,000 writes/sec
- Multiple API Servers load balanced
```

---

## Monitoring & Observability

### What to Monitor

**etcd Health:**
```
âœ… Leader elections (should be rare)
âœ… Database size (should grow slowly)
âœ… Disk latency (should be <10ms)
âœ… Network latency between nodes (<50ms)
âœ… Failed proposals (should be 0)
âœ… Compaction happening regularly
```

**API Server Health:**
```
âœ… Request latency (p99 < 1 second)
âœ… Error rate (< 0.1%)
âœ… Authentication failures
âœ… Rate limiting events
âœ… Watch cache hit rate (> 90%)
âœ… Memory usage
```

---

### Key Metrics to Watch

**etcd:**
```
etcd_server_has_leader: 1 (healthy) or 0 (problem!)
etcd_disk_backend_commit_duration: <25ms (good)
etcd_disk_wal_fsync_duration: <10ms (good)
etcd_network_peer_round_trip_time: <50ms (good)
etcd_mvcc_db_total_size_in_bytes: Growing slowly âœ“
```

**API Server:**
```
apiserver_request_duration_seconds: p99 < 1s
apiserver_request_total: Count by code
apiserver_current_inflight_requests: < 400
apiserver_longrunning_gauge: Active watch connections
```

---

## Common Problems & Solutions

### Problem 1: etcd Database Too Large

**Symptoms:**
- Slow writes
- High disk usage
- Backup takes forever
- Restore takes forever

**Causes:**
```
Too many events stored: 
- Kubernetes keeps events for 1 hour
- But accumulate quickly
- 1000 pods Ã— 10 events = 10,000 events

Too many old revisions:
- etcd keeps history
- Never cleaned up
- Database grows infinitely
```

**Solutions:**
```
1. Enable automatic compaction
   etcd --auto-compaction-retention=1h

2. Defragment regularly
   etcdctl defrag

3. Reduce event TTL
   kube-apiserver --event-ttl=30m

4. Clean up unused resources
   kubectl delete pods --field-selector status.phase=Succeeded
```

---

### Problem 2: API Server Overloaded

**Symptoms:**
- kubectl commands slow/timeout
- Applications can't deploy
- "connection refused" errors

**Causes:**
```
Too many watch connections:
- Every controller watches resources
- Each watch = persistent connection
- 1000 controllers Ã— 10 watches = 10,000 connections

Too many list requests:
- "kubectl get pods --all-namespaces" expensive
- Operators frequently listing resources
- No pagination used
```

**Solutions:**
```
1. Use pagination
   kubectl get pods --limit=500

2. Use field selectors
   kubectl get pods --field-selector status.phase=Running

3. Cache on client side
   Don't repeatedly list same data

4. Run multiple API Servers
   3-5 instances behind load balancer
```

---

### Problem 3: Slow etcd Writes

**Symptoms:**
- Deployments take minutes instead of seconds
- "context deadline exceeded" errors
- High API Server latency

**Causes:**
```
Slow disks:
- Using HDD instead of SSD
- Network-attached storage with latency
- Disk I/O saturated

High network latency:
- etcd nodes in different regions
- >50ms latency between nodes
- Raft consensus slowed down
```

**Solutions:**
```
1. Use fast local SSDs
   NVMe SSD: ~100,000 IOPS
   Regular SSD: ~10,000 IOPS
   HDD: ~100 IOPS âŒ DON'T USE

2. Colocate etcd nodes
   Same data center
   Same rack if possible
   <5ms network latency

3. Tune etcd
   --heartbeat-interval=100
   --election-timeout=1000
```

---

## Key Takeaways

### The 5 Most Important Things

**1. API Server is the Gateway**
- Everything goes through it
- No backdoors
- Handles authentication, authorization, validation
- Can scale horizontally (multiple instances)

**2. etcd is the Source of Truth**
- Stores all cluster state
- If etcd dies, cluster dies
- Must be backed up regularly
- Requires fast storage (SSD)

**3. They Work as a Team**
```
Write: You â†’ API Server â†’ etcd
Read: You â†’ API Server (cache) â†’ etcd (if not cached)
Watch: etcd â†’ API Server â†’ Controllers
```

**4. High Availability is Critical**
```
Production Setup:
- 3+ API Servers
- 3 or 5 etcd nodes (odd number)
- Load balancer in front
- Regular backups
```

**5. Performance Matters**
```
Fast etcd = Fast cluster
Slow etcd = Slow everything

Invest in:
âœ… Fast SSDs (NVMe preferred)
âœ… Low-latency network
âœ… Regular maintenance (compaction)
âœ… Monitoring
```

---

## Tomorrow (Day 3)

We'll explore:
- Scheduler in detail
- How pods get placed on nodes
- Affinity, taints, tolerations
- Resource requests and limits
- What happens when resources run out

**You now understand the brain (API Server) and memory (etcd) of Kubernetes! These two components work together to keep your cluster running smoothly.** ğŸ§ ğŸ’¾